{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Neural Networks.ipynb","provenance":[],"authorship_tag":"ABX9TyO3YtHvuAfdTXEVCINhbv7b"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"VitK0QnFKAv6","colab_type":"text"},"source":["**Introduction to Neural Networks**\n","\n","In this notebook you will learn how to create and use a neural network to classify articles of clothing. To achieve this, we will use a sub module of TensorFlow called keras.\n","\n","This guide is based on the following TensorFlow documentation.\n","\n","https://www.tensorflow.org/tutorials/keras/classification"]},{"cell_type":"markdown","metadata":{"id":"YyMYumJlKJTs","colab_type":"text"},"source":["**Keras**\n","\n","Before we dive in and start discussing neural networks, I'd like to give a breif introduction to keras.\n","\n","From the keras official documentation (https://keras.io/) keras is described as follows.\n","\n","\"Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano. It was developed with a focus on enabling fast experimentation.\n","\n","Use Keras if you need a deep learning library that:\n","\n","Allows for easy and fast prototyping (through user friendliness, modularity, and extensibility).\n","Supports both convolutional networks and recurrent networks, as well as combinations of the two.\n","Runs seamlessly on CPU and GPU.\"\n","Keras is a very powerful module that allows us to avoid having to build neural networks from scratch. It also hides a lot of mathematical complexity (that otherwise we would have to implement) inside of helpful packages, modules and methods.\n","\n","In this guide we will use keras to quickly develop neural networks."]},{"cell_type":"markdown","metadata":{"id":"r3yr9TKDKgD7","colab_type":"text"},"source":["**What is a Neural Network**\n","\n","So, what are these magical things that have been beating chess grandmasters, driving cars, detecting cancer cells and winning video games?\n","\n","A deep neural network is a layered representation of data. The term \"deep\" refers to the presence of multiple layers. Recall that in our core learning algorithms (like linear regression) data was not transformed or modified within the model, it simply existed in one layer. We passed some features to our model, some math was done, an answer was returned. The data was not changed or transformed throughout this process. A neural network processes our data differently. It attempts to represent our data in different ways and in different dimensions by applying specific operations to transform our data at each layer. Another way to express this is that at each layer our data is transformed in order to learn more about it. By performing these transformations, the model can better understand our data and therefore provide a better prediction."]},{"cell_type":"markdown","metadata":{"id":"MEUUA43AgsDI","colab_type":"text"},"source":["##How it Works\n","\n","Before going into too much detail, I will provide a very surface level explaination of how neural networks work on a mathematical level. All the terms and concepts, will be defined and explained in more detail below.\n","\n","On a lower level neural networks are simply a combination of elementry math operations and some more advanced linear algebra. Each neural network consists of a sequence of layers in which data passes through. These layers are made up of neurons and the neurons of one layer are connected to the next (see below). These connections are defined by what we call a weight (some numeric value). Each layer also has something called a bias, this is simply an extra neuron that has no connections and holds a single numeric value. Data starts at the input layer and is transformed as it passes through subsequent layers. The data at each subsequent neuron is defined as the following.\n","\n","> $Y =(\\sum_{i=0}^n w_i x_i) + b$\n","\n","> $w$ stands for the weight of each connection to the neuron\n","\n","> $x$ stands for the value of the connected neuron from the previous value\n","\n","> $b$ stands for the bias at each layer, this is a constant\n","\n","> $n$ is the number of connections\n","\n","> $Y$ is the output of the current neuron\n","\n","> $\\sum$ stands for sum\n","\n","The equation you just read is called a weighed sum. We will take this weighted sum at each and every neuron as we pass information through the network. Then we will add what's called a bias to this sum. The bias allows us to shift the network up or down by a constant value. It is like the y-intercept of a line.\n","\n","But that equation is the not complete one! We forgot a crucial part, **the activation function**. This is a function that we apply to the equation seen above to add complexity and dimensionality to our network. Our new equation with the addition of an activation function $F(x)$ is seen below.\n","\n","> $Y =F((\\sum_{i=0}^n w_i x_i) + b)$\n","\n","Our network will start with predefined activation functions (they may be different at each layer) but random weights and biases. As we train the network by feeding it data it will learn the correct weights and biases and adjust the network accordingly using a technqiue called **backpropagation** (explained below). Once the correct weights and biases have been learned our network will hopefully be able to give us meaningful predictions. We get these predictions by observing the values at our final layer, the output layer. \n"]},{"cell_type":"markdown","metadata":{"id":"xr88EwvxhVlX","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"oT-7bpeQKYZi","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VCR_rj33KF9j","colab_type":"text"},"source":[""]}]}